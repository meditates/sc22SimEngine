\section{Related Work}
\label{sec:relatedWork}
\subsection{Proxy Application Characterization}
Proxy application characterization, including the comparison of individual proxies to their parent applications~\cite{BARRETT2015107, Islam:2016:MLF:3014904.3014966, 8049024, CPE:CPE3587} and performance projection~\cite{barrett2012navigating, sharkawi2009performance,8049025} have been well studied. We present the most recent related work below. 

Aaziz et al.~\cite{8514880} applied hierarchical clustering to a small selected subset of hardware performance event counters across proxy/parent pairs to understand behavior similarity at the node level. Although this method accurately clusters proxy/parent pairs that are similar, it provides more of a relative measure of similarity between similar proxy/parent pairs.  The similarity is measured by tree branch height, which has little physical
meaning. Additionally, this method did not provide a visual representation that enabled a quick understanding of the range of application behavior represented (or not) by the suite of proxy applications used. Therefore, eliminating proxies with redundant behavior was not easily supported by this method.

Tramm et al. ~\cite{XSBench} perform a characterization of OpenMC, and its proxy, XSBench. Multi-core scaling efficiency, floating point calculation rates, and hardware performance counter profiles are correlated to an efficiency loss metric that is related to performance as the number of threads is increased.  This work is a study on a single proxy/parent application pair and presents a methodology that is manual rather than machine-learning based. They focus more on the characterization of performance, particularly for proxies, and they present similarity of proxies, rather than similarity of proxy/parent pairs.  Kim et al.~\cite{8049024} extend their KGen Fortran Kernel Generator tool with the ability to gather descriptive statistics from both the original application and the KGen kernel extracted from the instrumented application. They do not evaluate proxies, but rather extract a kernel from an application, and apply general descriptive statistics.

Barrett et al.~\cite{BARRETT2015107,CPE:CPE3587} are active in developing proxy applications and in assessing their characteristics with real applications. In~\cite{CPE:CPE3587} they do an in-depth analysis of one real and proxy application pair (Charon/miniFE), while in~\cite{BARRETT2015107} they propose a methodology whereby a set of performance domains is considered for some pair of proxy and real applications, and comparison and threshold functions are created for each performance domain.  For example, for the LAMMPS/miniMD pair (molecular dynamics), their domains are total time, force calculation time, neighbor list construction time, and inter-process communication time. While the abstract methodology framework is common, since the instantiation for each pair of applications is unique, their work is somewhat orthogonal to ours, as we are proposing a common instantiated methodology.

Islam et al. ~\cite{Islam:2016:MLF:3014904.3014966} created the Veritas framework to measure proxy/parent relationships using belief estimation in Dempster-Shafer theory over low level resource measurements of CPU component usage. They then computed the amount that a proxy \emph{covers} the real application in each of the resource categories. The resource categories they used were floating point, branch execution, TLB, L1-L3 caches, memory, and prefetching. They also used PCA to achieve some dimensionality reduction in their data.

Tsuji et al.~\cite{8049025} address the use of proxy applications to inform the use of benchmark applications as a precursor in assessing expected system performance. They propose a new metric, SSSP or Simplified Sustained System Performance, based on a prior SSP method and calculated over kernel benchmarks rather than real applications. This adapted methodology avoids the use of real applications in establishing system performance, which is known to be costly to set up and tune to a given system. They demonstrate the consistency of SSSP with SSP while only utilizing data from proxy applications and simple benchmark kernels. A similar form of performance projection is likewise presented by Sharkawi et al.~\cite{sharkawi2009performance}. Dickson et al.~\cite{7836562} extract an I/O pattern from an application and recreate that pattern in a proxy application which is used to explore alternative I/O library paradigms. This work is specific to I/O behavior and does not evaluate existing proxies, but is valuable since existing proxies often ignore I/O behavior.

%Aaziz et al.~\cite{aaziz2018methodology} applied hierarchical clustering to a small selected subset of hardware performance event counters across proxy/parent pairs to understand behavior similarity at the node level.
\subsection{Distance Metrics and Feature Selection}
Other cosine similarity metrics including Square Root Cosine Similarity and Improved Square Root Cosine Similarity have also been studied in ~\cite{sohangir2017improved}, however, neither reflects identity in the case of comparing a vector with itself nor does either present a clear geometric understanding of the comparison. There are more similarity algorithms available, but not included in this paper. Our work serves as a metaanalysis across similarity methods, demonstrating their interchangeability in this domain. 

Distance metric learning is a branch of machine learning that aims to learn distances from
the data, which enhances the performance of similarity-based algorithms~\cite{suarez2021tutorial}. Automatically learning customized metrics from data~\cite{bellet2013survey} has attracted a lot o interest in recent years. \eg given some examples of similar pairs of points, a linear transformation matrix (Mahalanobis distance metric) is learned to represent these relationships~\cite{xing2002distance,weinberger2009distance}. However, these approaches have an assumption that similarity or dissimilarity is clearly defined, while in our problem this is not the case.


Most existing unsupervised feature selection algorithms such as Relief~\cite{kira1992practical} and ReliefF~\cite{kononenko1997overcoming}, Laplacian score~\cite{he2005laplacian}, Fisher Score~\cite{jennrich1976newton}, SPEC~\cite{nguyen2014effective}, HSIC~\cite{yamada2012high}, and Trace Ratio~\cite{nie2008trace}, are essentially based on assessing features' capability in preserving similarity. However, a common drawback is being unable to handle feature redundancy~\cite{zhao2011similarity}. In Lindenbaum \etal~\cite{lindenbaum2021differentiable}, nuisance features are filtered before computation of importance to improve the accuracy and efficiency. Yu \etal~\cite{yu2003feature} proposes a correlation-based filter to remove the redundancy in a supervised scenario.
PFA~\cite{lu2007feature} exploits the structure of the principal components by choosing the important features with the nearest distance to the cluster center, but the assumption of normalized distribution for k-means may not be true in some circumstances. Li \etal~\cite{li2018feature} reviews some feature selection algorithms for structured data where features exhibit group, tree, or graph structures, but most of them need prior knowledge of structures, which is still a challenging problem for automatic feature selection.
