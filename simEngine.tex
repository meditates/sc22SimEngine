\section{\us}
\label{sec:simEngine}
%Our fundamental question in this work is 
To determine how similar proxy and parent applications behave in terms of resource utilization such as computation and memory, we explore the use of several ML similarity techniques, which are all unsupervised. These techniques work differently, making it difficult to determine which one reveals the real relationship. Therefore, we create \us to interpret the level of agreement between these techniques and converge upon a measure of proxy fidelity. 
%\todo{why we need to use several similarity techniques?}
%A fundamental question we pose is: ``how similar are applications to each other in the way they utilize system resources, particularly computation and memory?''
%, with a focus in this work on node computation and memory resources. 
%We introduce a framework \us to determine this similarity between proxy and parent applications.
%Figure~\ref{figs:us} shows the workflow of \us.

We collect hardware performance counters from several proxy applications, parent applications, and benchmarks to provide a comprehensive collection of informational measurements that reveal application execution resource fingerprints to determine similarity.  
Figure~\ref{figs:us} shows the workflow of \us, where we use Lightweight Distributed Metric Service (LDMS)~\cite{ldms_sandia} as the data collection framework to collect PAPI hardware performance counters from HPC applications. This data is processed (see Section~\ref{sec:prep}) and then input to the feature selection layer as a matrix $X$ that has $n$ rows of application vectors $x_{1},\ldots,x_{n}$ and features $f_{1},\ldots,f_{d}$ as the columns.

%a vector $x_{i}$ with the length of $d$ dimensions ($d$ being the number of hardware events), and build a data matrix $X$ that has $n$ rows of application vectors $x_{1},\ldots,x_{n}$. The columns of $X$ are the features $f_{1},\ldots,f_{d}$.

During feature selection, we rank the features by calculating importance scores and selecting the important $k$ features using a correlation filter.
The matrix $X'$, which has $k$ features as the columns, preserves the similarity structure of the matrix $X$. 
Finally, using the selected important features, we produce similarity matrices to compare the similarity between applications, and quantify the fidelity of proxy and parent applications. 
%Our analysis tools that compute feature ranking, correlation filter, and similarity measurement are written in python and use several of the standard math libraries (\eg math, numpy) and scikit-learn facilities.  We have also developed an extensive infrastructure to automate data collection experiments.\todo{possibly move this paragraph to artifact evaluation}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figs/SimEngine.png}
\caption{\us Architecture }
\label{figs:us}
\end{figure}

\subsection{Similarity and Distance}
\label{sec:Sim}

Evaluating the similarity between proxy and parents is one of the main goals of our work.
For each application, we sample, by the second, the accumulated hardware counters, for the whole execution. Then we average the last 5 seconds of the application execution for each event across ranks. Thus, we get an application vector $x_{i}$, that contains a series of averaged hardware event counters (\ie $x_{ik}$). 
%\todo{sentence about how we represent applciations as vectors} .
We define two applications as \emph{similar} if the vectors that represent the applications are a short distance apart, which, in many classification tasks, corresponds to the vectors belonging to the same class. %The similarity is defined using a distance measure. 
After calculating the pairwise distance between each application pair, we get a similarity matrix. %We expect the diagonal of the matrix to be zero, as that the distance of the vectors between the applications and themselves. 
Finally, we compare the results of four typical similarity metrics, introduced below, and choose the metric that most accurately correlates known proxy/parent pairs. 

\subsubsection{Cosine Similarity}
\label{sec:cos_sim}
Cosine similarity compares the angle between vectors in an inner product space, where the inner product can be conceptualized
as the projection of one vector $x_{i}$ in the direction of the other vector $x_{j}$. The calculation relies on two complementary definitions (algebraic and geometric) for computing the inner product:
\begin{itemize}
\item 
Algebraic Inner Product:
$x_{i}\cdot x_{j}=\sum_{k=1}^{d} x_{ik} x_{jk}$
\item
Geometric Inner Product:
$x_{i}\cdot x_{j}=\|x_{i}\|\|y_{i}\|\cos\theta$
\item
Cosine Similarity:
\begin{equation}
\cos(\theta)=\frac{\sum_{k=1}^{d} x_{ik} x_{jk}}{\|x_{i}\|\|x_{j}\|}
\end{equation}
\end{itemize} 
%Cosine similarity compares the angle between two vectors $x_{i}$ and $x_{j}$ using vector inner product. 
The cosine varies from 1.0 (identical vector direction) to 0.0 (orthogonal vectors), and the degree $\theta$ varies from $0^\circ$ (similar) to $90^\circ$ (dissimilar). If two applications have similar behaviors then we are expecting their cosine similarity angle to be closer to $0^\circ$.  

\subsubsection{Jensen-Shannon (JS) Divergence}
\label{sec:JS}
Instead of comparing two vectors, we can normalize each vector and think of it as a probability distribution (each event turns into a probability), and then compare the two distributions.
JS divergence~\cite{endres2003new} measures the distance between two probability distributions $P$ and $Q$. JS divergence is a generalization of  Kullback–Leibler (KL) divergence~\cite{kullback1951information}, the relative entropy from distributions $Q$ to $P$, \ie the expectation of the logarithmic difference between the probability distributions $Q$ and $P$. 
\begin{align*}
KL(P|Q)&=\sum_{x} P(x) \log \frac{P(x)}{Q(x)} \\ 
&=-\sum_{x} P(x) \log {Q(x)}+\sum_{x} P(x) \log {P(x)} \\ 
& =\textrm{cross entropy – entropy}
\end{align*}
KL divergence is asymmetric and has no upper bound. Unlike KL divergence, JS divergence is symmetric and returns a value between 0 and 1, where 0 is similar and 1 is divergent. %both of which are valuable properties for application comparison. 
JS divergence is defined as 
\begin{equation}
\operatorname{JS}(P \| Q)=\frac{1}{2} KL(P \| M)+\frac{1}{2} KL(Q \| M)
\end{equation}
where $M=\frac{1}{2}(P+Q)$.% JS divergence calculates total KL divergences in two directions to the average.  

\subsubsection{Wasserstein Distance}
\label{sec:wass}
Wasserstein distance\cite{rubner2000earth} can be interpreted as the minimum ``cost'' of transforming a probability distribution $P$ into distribution $Q$, where ``cost'' is measured as the amount of distribution weight that must be moved, times the distance it has to be moved~\cite{wassersteinscipy}. 
Unlike other statistical distances like JS divergence, Wasserstein distance multiplies the probability distribution and the distance, thus the order of the events matters. 
%If the events that have divergent behaviour located far away in the vectors, the Wasserstein distance is bigger. 
Since Wasserstein distance does not require both measures to be in the same probability space (\ie two vectors may have different lengths), it can be used to compare application performance across platforms that support a different number of hardware event counts. The $p^{th}$ Wasserstein distance is defined as 
\begin{equation}
W_{p}(P, Q)=\left(\inf _{J \in \mathcal{J}(P, Q)} \int\|x-y\|^{p} d J(x, y)\right)^{1 / p}
\end{equation},
where $\mathcal{J}(P, Q)$ denotes all joint distributions $J$ for $(X,Y)$ that have marginals $P$ and $Q$. 
We use the first Wasserstein distance ($p$ = 1) between two 1-dimensional distributions, which is also known as earth mover distance, to determine similarity. Wasserstein distance does not have an upper bound; 0 indicates similarity (equivalence), while increasing values indicate growing divergence. 
%Generally, when $p = 1$ and $d = 1$, we compute the first Wasserstein distance between two 1-dimensional distributions, which is also known as earth mover distance. Wasserstein distance does not have upper bound, where 0 means similar and bigger value means dissimilar.

\subsubsection{Mahalanobis Distance}
Mahalanobis distance~\cite{Mah} is an effective multivariate distance metric that measures the distance between a point (vector) and a distribution, or two points from the same distribution. 
The Mahalanobis distance between two vectors $x_i$ and $x_j$ from the same distribution is defined as
\begin{equation}d_{M}(x_i, x_j)=\sqrt{(x_i-x_j)^{T} S^{-1}(x_i-x_j)}
\end{equation}, where $S$ is the covariance matrix of the data set. Geometrically, it transforms the data by whitening and normalizing the covariance, and computes the Euclidean distance for the transformed data. Since the Mahalanobis distance accounts for the variance of each variable and the covariance between variables, it is used in areas such as multivariate anomaly detection and imbalanced classification. Note that the Mahalanobis distance requires more samples than features in order to calculate the covariance matrix. Thus, we reduce dimensionality with principal components analysis (PCA) before applying Mahalanobis distance. Mahalanobis distance also does not have an upper bound; here, 0 indicates similarity (equivalence), while increasing values indicate growing divergence. 

\subsection{Feature Selection}
%The data collection process is torture in our experiment. Without knowing which hardware events are more representative, we exhaustively collected all the events that the platforms could provide, which is around 500 hardware events for each application on platform Intel Skylake. 
A large number of hardware performance counter events are available on most HPC architectures, making it difficult to identify a comprehensive but minimal set. Thus, we exhaustively collect all the events available on the Intel Skylake platform (500 total events) for each application.
%Occam’s Razor shows that the simplest model that explains a situation is the correct approach. 
However, our aim is to reduce irrelevant features, since these features may add noise, reduce model accuracy, increase latency and storage overhead, or extend the amount of data processing. 
%There are two classes of algorithms to reduce a feature space: feature extraction and feature selection. 
%feature extraction, which recombines original features into a denser set, retains the overhead of data collection and sacrifices interpretability, so we focus on feature selection.
Therefore, we have to find a suitable feature selection algorithm that is simple and efficient, in order to speed up our engine, 
%improves accuracy, and enhances comprehensibility. 
%Feature extraction is to find a smaller set of new variables, each being a combination of the original features. Because feature extraction still requires collecting all the features and loses interpretability, it is not what we are looking for. 
%\avani{I propose we cut the rest of this paragraph}
%Feature Selection can be categorized into filter methods, wrapper methods, and embedded methods. The filter methods always focus on the intrinsic properties (\eg Variance) of data itself, which are more suitable for unsupervised learning. %Examples of filter methods include Information Gain, Chi-square Test, Fisher’s Score, Correlation Coefficient, Variance Threshold, Mean Absolute Difference, and Dispersion ratio. 
%Wrapper methods evaluate features by applying a selection algorithm and comparing the final value with the target value. They require a greedy search in the feature space. The typical wrapper method is Recursive Feature Elimination\cite{guyon2002gene}. Embedded methods, such as Random Forest\cite{ho1995random}, get the important features while interactively building up the model. Most of the wrapper and embedded feature selection methods are designed for supervised learning. 
%find a concise event subset, simplify the data collection, and enable cross-platform comparison. \us uses Laplacian score~\cite{he2005laplacian} to rank the important features, combined with the Pearson correlation to remove the correlated features.
find a concise event subset of uncorrelated features, simplify the data collection, and enable cross-platform comparison. There are two components in the feature selection layer in \us: feature score, which is used to rank the important features, and the correlation filter, which is used to remove the correlated features. 

\subsubsection{Feature Score}
\label{sec:feature_score}
Consider a data matrix $X$ that has $n$ rows of samples $x_{1},...,x_{n}$ with $d$ features $f_{1},...f_{d}$. Our goal is to find the subset of $k$ important features that preserve the similarity structure of the matrix $X$. 

%Since we lack ground truth for the class label for each application, the filter method of feature selection is more suitable.
Since we are mainly interested in maintaining the similarity relationship between the proxy/parent application pairs, neighbor embedding is the perfect unsupervised method to reduce our feature set. We choose a graph-based feature ranking technology called Laplacian score~\cite{he2005laplacian} to compute the importance of features.  Laplacian score builds a nearest neighbor graph for application points and seeks those features that respect local graph structure. In our case, these features help preserve the neighbor similarity between proxy and parent applications.  

We express the Laplacian score of the $r^\textrm{th}$ feature as:
\begin{equation}L_{r}=\frac{\widetilde{\mathbf{f}}_{r}^{T} L \widetilde{\mathbf{f}}_{r}}{\widetilde{\mathbf{f}}_{r}^{T} \widetilde{\mathbf{f}_{r}}}
\end{equation}
, or in a more understandable way:
\begin{equation}
\label{eqn:laplace}
L_{r}=\frac{\Sigma_{i j}\left(f_{r i}-f_{r j}\right)^{2} S_{i j}}{\operatorname{Var}\left(\mathbf{f}_{r}\right)}
\end{equation} , where $f_r$ is the $r^\textrm{th}$ feature, and $S_{i j}$ is the weight matrix.  
\begin{equation}S_{i j}=e^{-\frac{\left\|\mathrm{x}_{i}-\mathrm{x}_{j}\right\|^{2}}{2}}\end{equation}
An element $S_{i j}$ will only have a nonzero value when $i$ and $j$ are neighbors, otherwise the value of that entry is zero. The denominator in Eqn.~\ref{eqn:laplace} indicates the variance of the $r^\textrm{th}$ feature; larger values correlate to more information represented. The numerator indicates the sum of feature value differences within the points' neighbors; here, the smaller the Laplacian score is, the more important the feature is. Finally, we get the ranked features sorted by Laplacian score.

Since features are evaluated separately in the Laplacian score, if we select features with the smallest Laplacian score, some correlated features may be included. To further reduce the important feature set,  we introduce the correlation filter.

\subsubsection{Correlation Filter}
When a group of features are highly correlated, we only need to include one in our selection. A broadly used correlation measure is the Pearson correlation coefficient (PCC). PCC measures the linear correlation between two variables $f_{i}, f_{j}$. 
\begin{equation}
\rho_{f_{i}, f_{j}}=\frac{\operatorname{cov}(f_{i}, f_{j})}{\sigma_{f_{i}} \sigma_{f_{j}}}
\end{equation}
where $\operatorname{cov}$ is the covariance and $\sigma_{f_{i}}$is the standard deviation of $f_{i}$.
PCC has a value between +1 and -1, where +1 is a total positive linear correlation, 0 is no linear correlation, and -1 is a total negative linear correlation. We choose a PCC threshold of 0.9 because we want to keep a reasonable number of important features while removing those with strong correlation.  

We calculate the pairwise PCC for all ranked features and extract features sequentially from the ranked feature set and add them to our important feature subset. Each time we select one feature, we remove the redundant features with PCC $> 0.9$ or $< -0.9$ from the remaining features. We stop collecting features when there are no more features in the feature pool. %we obtain a pre-selected number, or when there are no more features in the feature pool.

Note that the PCC can evaluate only a linear relationship between two continuous variables; we will investigate more correlation methods in future work.